{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN/O4sn3xNhdZcyVasqdY82",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kziyang1207/project/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uvhoyo2iN3kL",
        "outputId": "70d2ce8f-7ed8-41f8-e07d-e32fe4d3f8c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hTorch: 2.8.0+cu126\n",
            "Transformers: 4.56.0\n",
            "CUDA available: True\n",
            "Sat Aug 30 06:22:27 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!pip -q install -U \"transformers>=4.40\" datasets evaluate accelerate scikit-learn\n",
        "\n",
        "import torch, transformers\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"Transformers:\", transformers.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    !nvidia-smi\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EpUDvNzoT5QR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# üëá change this to your folder path\n",
        "PATH_PREFIX = \"/content\"\n",
        "\n",
        "TRAIN_CSV = f\"{PATH_PREFIX}/reviews_train.csv\"\n",
        "VAL_CSV   = f\"{PATH_PREFIX}/reviews_val.csv\"\n",
        "TEST_CSV  = f\"{PATH_PREFIX}/reviews_test.csv\"\n",
        "\n",
        "import os\n",
        "for p in [TRAIN_CSV, VAL_CSV, TEST_CSV]:\n",
        "    assert os.path.exists(p), f\"Missing file: {p}\"\n",
        "print(\"All CSVs found ‚úÖ\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttyslR4zQHG8",
        "outputId": "effb9b56-a858-4b1b-f4d8-2c30e16280df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "All CSVs found ‚úÖ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_bert.py\n",
        "# GPU-only DistilBERT fine-tuning for 5-class review labeling (Colab, robust to HF versions)\n",
        "# CSV columns required: [\"combined_text\", \"label\"] where label is STRING class.\n",
        "\n",
        "import os, argparse, inspect\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # avoid fork+tokenizers warning\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    TrainingArguments, Trainer, DataCollatorWithPadding\n",
        ")\n",
        "import evaluate\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch\n",
        "\n",
        "def load_splits(train_csv, val_csv, test_csv):\n",
        "    train_df = pd.read_csv(train_csv); val_df = pd.read_csv(val_csv); test_df = pd.read_csv(test_csv)\n",
        "    for name, df in [(\"train\", train_df), (\"val\", val_df), (\"test\", test_df)]:\n",
        "        if \"combined_text\" not in df.columns or \"label\" not in df.columns:\n",
        "            raise ValueError(f\"{name} must contain ['combined_text','label']\")\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    \"\"\"Drops odd keys and uses class-weighted CE loss (GPU-only).\"\"\"\n",
        "    def __init__(self, class_weights: np.ndarray, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        inputs = dict(inputs)\n",
        "        inputs.pop(\"num_items_in_batch\", None)\n",
        "\n",
        "        # üîß FIX: do NOT use `or` on tensors; check None sequentially\n",
        "        labels = inputs.pop(\"labels\", None)\n",
        "        if labels is None:\n",
        "            labels = inputs.pop(\"label\", None)\n",
        "        if labels is None:\n",
        "            labels = inputs.pop(\"label_ids\", None)\n",
        "\n",
        "        allowed = {\"input_ids\",\"attention_mask\",\"token_type_ids\",\"return_dict\",\n",
        "                   \"output_attentions\",\"output_hidden_states\",\"position_ids\",\"head_mask\",\"inputs_embeds\"}\n",
        "        model_inputs = {k:v for k,v in inputs.items() if k in allowed}\n",
        "\n",
        "        outputs = model(**model_inputs)\n",
        "        logits = outputs.logits if hasattr(outputs,\"logits\") else outputs[0]\n",
        "\n",
        "        if labels is not None:\n",
        "            w = self.class_weights.to(logits.device)\n",
        "            loss = torch.nn.functional.cross_entropy(logits, labels.to(logits.device), weight=w)\n",
        "        else:\n",
        "            loss = getattr(outputs,\"loss\", None) or torch.zeros((), device=logits.device)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "def auto_mp_kwargs():\n",
        "    \"\"\"Prefer bf16 on Ampere+; else fp16 on CUDA; else none.\"\"\"\n",
        "    mp = {}\n",
        "    params = set(inspect.signature(TrainingArguments.__init__).parameters.keys())\n",
        "    caps = None\n",
        "    if torch.cuda.is_available():\n",
        "        try: caps = torch.cuda.get_device_capability(0)[0]\n",
        "        except Exception: caps = None\n",
        "    if \"bf16\" in params and caps is not None and caps >= 8:\n",
        "        mp[\"bf16\"] = True\n",
        "    elif \"fp16\" in params and torch.cuda.is_available():\n",
        "        mp[\"fp16\"] = True\n",
        "    return mp\n",
        "\n",
        "def build_training_args(cli, outdir):\n",
        "    \"\"\"Create TrainingArguments; keep eval/save strategies consistent; disable ES if unsupported.\"\"\"\n",
        "    params = set(inspect.signature(TrainingArguments.__init__).parameters.keys())\n",
        "    has_eval_strategy = \"evaluation_strategy\" in params\n",
        "    has_save_strategy = \"save_strategy\" in params\n",
        "\n",
        "    kw = dict(\n",
        "        output_dir=outdir,\n",
        "        per_device_train_batch_size=cli.train_batch_size,\n",
        "        per_device_eval_batch_size=cli.eval_batch_size,\n",
        "        gradient_accumulation_steps=cli.grad_accum,\n",
        "        num_train_epochs=cli.epochs,\n",
        "        learning_rate=cli.lr,\n",
        "        logging_steps=cli.logging_steps,\n",
        "        save_total_limit=2,\n",
        "        seed=42,\n",
        "    )\n",
        "\n",
        "    # Ensure evaluation happens\n",
        "    if has_eval_strategy:\n",
        "        kw[\"evaluation_strategy\"] = \"steps\"\n",
        "        if \"eval_steps\" in params: kw[\"eval_steps\"] = cli.eval_steps\n",
        "    else:\n",
        "        kw[\"do_eval\"] = True  # legacy fallback\n",
        "\n",
        "    # Save strategy\n",
        "    if has_save_strategy:\n",
        "        kw[\"save_strategy\"] = \"steps\"\n",
        "        if \"save_steps\" in params: kw[\"save_steps\"] = cli.save_steps\n",
        "    else:\n",
        "        if \"save_steps\" in params: kw[\"save_steps\"] = cli.save_steps  # legacy fallback\n",
        "\n",
        "    # Extras\n",
        "    if \"weight_decay\" in params: kw[\"weight_decay\"] = 0.01\n",
        "    if \"warmup_ratio\" in params: kw[\"warmup_ratio\"] = 0.06\n",
        "    if \"report_to\" in params: kw[\"report_to\"] = \"none\"\n",
        "    if \"remove_unused_columns\" in params: kw[\"remove_unused_columns\"] = False\n",
        "    if \"optim\" in params: kw[\"optim\"] = \"adamw_torch\"\n",
        "\n",
        "    # Avoid forking issues in Colab by using 0 workers (safe & quiet)\n",
        "    if \"dataloader_num_workers\" in params: kw[\"dataloader_num_workers\"] = 0\n",
        "    if \"dataloader_pin_memory\" in params: kw[\"dataloader_pin_memory\"] = True\n",
        "    if \"disable_tqdm\" in params: kw[\"disable_tqdm\"] = False\n",
        "\n",
        "    # Mixed precision\n",
        "    kw.update(auto_mp_kwargs())\n",
        "    if \"no_cuda\" in params: kw[\"no_cuda\"] = not torch.cuda.is_available()\n",
        "\n",
        "    # Provide metric fields if available (for callbacks/best model)\n",
        "    if \"metric_for_best_model\" in params: kw[\"metric_for_best_model\"] = \"f1_macro\"\n",
        "    if \"greater_is_better\" in params: kw[\"greater_is_better\"] = True\n",
        "\n",
        "    # Best model loading only if user requests AND both strategies exist\n",
        "    if \"load_best_model_at_end\" in params:\n",
        "        kw[\"load_best_model_at_end\"] = False\n",
        "        if cli.load_best and has_eval_strategy and has_save_strategy:\n",
        "            kw[\"load_best_model_at_end\"] = True\n",
        "\n",
        "    return TrainingArguments(**kw)\n",
        "\n",
        "def maybe_subsample(df, nmax):\n",
        "    if nmax and nmax > 0 and len(df) > nmax: return df.sample(n=nmax, random_state=42).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "def main(cli):\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"‚ö†Ô∏è No GPU detected. Enable GPU in Colab runtime for speed.\")\n",
        "\n",
        "    # 1) Load & (optionally) subsample\n",
        "    train_df, val_df, test_df = load_splits(cli.train_csv, cli.val_csv, cli.test_csv)\n",
        "    train_df = maybe_subsample(train_df, cli.max_train_samples)\n",
        "    val_df   = maybe_subsample(val_df,   cli.max_eval_samples)\n",
        "    test_df  = maybe_subsample(test_df,  cli.max_eval_samples)\n",
        "\n",
        "    # 2) Encode labels\n",
        "    le = LabelEncoder().fit(train_df[\"label\"].tolist())\n",
        "    train_df[\"label_id\"] = le.transform(train_df[\"label\"])\n",
        "    val_df[\"label_id\"]   = le.transform(val_df[\"label\"])\n",
        "    test_df[\"label_id\"]  = le.transform(test_df[\"label\"])\n",
        "    train_df = train_df[[\"combined_text\",\"label_id\"]].rename(columns={\"label_id\":\"label\"}).reset_index(drop=True)\n",
        "    val_df   = val_df[[\"combined_text\",\"label_id\"]].rename(columns={\"label_id\":\"label\"}).reset_index(drop=True)\n",
        "    test_df  = test_df[[\"combined_text\",\"label_id\"]].rename(columns={\"label_id\":\"label\"}).reset_index(drop=True)\n",
        "    id2label = {i: lab for i, lab in enumerate(le.classes_)}\n",
        "    label2id = {lab: i for i, lab in id2label.items()}\n",
        "    num_labels = len(id2label)\n",
        "\n",
        "    # 3) Class weights\n",
        "    classes = np.arange(num_labels)\n",
        "    class_weights = compute_class_weight(\"balanced\", classes=classes, y=train_df[\"label\"].values).astype(\"float32\")\n",
        "\n",
        "    # 4) Datasets\n",
        "    train_ds = Dataset.from_pandas(train_df, preserve_index=False)\n",
        "    val_ds   = Dataset.from_pandas(val_df,   preserve_index=False)\n",
        "    test_ds  = Dataset.from_pandas(test_df,  preserve_index=False)\n",
        "\n",
        "    # 5) Tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cli.model_name)\n",
        "        # after: tokenizer = AutoTokenizer.from_pretrained(cli.model_name)\n",
        "\n",
        "\n",
        "    def tok(batch): return tokenizer(batch[\"combined_text\"], truncation=True, max_length=cli.max_length)\n",
        "    train_ds = train_ds.map(tok, batched=True).remove_columns([\"combined_text\"])\n",
        "    val_ds   = val_ds.map(tok, batched=True).remove_columns([\"combined_text\"])\n",
        "    test_ds  = test_ds.map(tok, batched=True).remove_columns([\"combined_text\"])\n",
        "    needed = [\"input_ids\",\"attention_mask\",\"label\"]\n",
        "    if \"token_type_ids\" in train_ds.column_names: needed.append(\"token_type_ids\")\n",
        "    for ds in (train_ds, val_ds, test_ds):\n",
        "        ds.set_format(type=\"torch\", columns=[c for c in needed if c in ds.column_names])\n",
        "    print(\"Sizes ‚Äî train/val/test:\", len(train_ds), len(val_ds), len(test_ds))\n",
        "\n",
        "    # 6) Model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        cli.model_name, num_labels=num_labels, id2label=id2label, label2id=label2id\n",
        "    )\n",
        "    special_tags = [\"[CATS]\", \"[RATING]\", \"[REVIEW]\"]\n",
        "    added = tokenizer.add_tokens(special_tags, special_tokens=False)\n",
        "    if added > 0:\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "    if cli.freeze_base:\n",
        "        frozen = False\n",
        "        for attr in [\"distilbert\",\"bert\",\"roberta\",\"albert\",\"deberta\",\"debertav2\",\"xlm_roberta\"]:\n",
        "            if hasattr(model, attr):\n",
        "                for p in getattr(model, attr).parameters(): p.requires_grad = False\n",
        "                frozen = True; break\n",
        "        if not frozen:\n",
        "            for name, p in model.named_parameters():\n",
        "                if \"classifier\" not in name and \"pre_classifier\" not in name: p.requires_grad = False\n",
        "        print(\"‚úÖ Base encoder frozen; training classifier head only.\")\n",
        "    if cli.gradient_checkpointing:\n",
        "        try:\n",
        "            if hasattr(model, \"gradient_checkpointing_enable\"): model.gradient_checkpointing_enable()\n",
        "            if hasattr(model.config, \"use_cache\"): model.config.use_cache = False\n",
        "            print(\"Gradient checkpointing enabled (slower but lower memory).\")\n",
        "        except: pass\n",
        "\n",
        "    collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    # 7) Metrics\n",
        "    acc = evaluate.load(\"accuracy\"); f1 = evaluate.load(\"f1\")\n",
        "    prec = evaluate.load(\"precision\"); rec = evaluate.load(\"recall\")\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        preds = np.argmax(logits, axis=-1)\n",
        "        return {\n",
        "            \"accuracy\": acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
        "            \"f1_macro\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
        "            \"precision_macro\": prec.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"],\n",
        "            \"recall_macro\": rec.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"],\n",
        "        }\n",
        "\n",
        "    # 8) Training args\n",
        "    outdir = cli.output_dir; os.makedirs(outdir, exist_ok=True)\n",
        "    training_args = build_training_args(cli, outdir)\n",
        "\n",
        "    # 9) Early stopping (attach only if the strategy is actually steps/epoch)\n",
        "    callbacks = []\n",
        "    try:\n",
        "        from transformers import EarlyStoppingCallback\n",
        "        can_es = False\n",
        "        strat = getattr(training_args, \"evaluation_strategy\", None) or getattr(training_args, \"eval_strategy\", None)\n",
        "        if strat is not None:\n",
        "            sval = str(strat).lower()\n",
        "            if (\"step\" in sval) or (\"epoch\" in sval):\n",
        "                can_es = True\n",
        "        if cli.early_stopping_patience > 0 and can_es:\n",
        "            callbacks.append(EarlyStoppingCallback(early_stopping_patience=cli.early_stopping_patience))\n",
        "        elif cli.early_stopping_patience > 0:\n",
        "            print(\"‚ÑπÔ∏è EarlyStopping disabled: evaluation strategy not supported in this HF version.\")\n",
        "    except Exception:\n",
        "        if cli.early_stopping_patience > 0:\n",
        "            print(\"‚ÑπÔ∏è EarlyStopping disabled: callback not available in this HF version.\")\n",
        "\n",
        "    # 10) Trainer\n",
        "    trainer = WeightedTrainer(\n",
        "        class_weights=class_weights,\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=callbacks,\n",
        "    )\n",
        "\n",
        "    # 11) Train & evaluate\n",
        "    trainer.train()\n",
        "    print(\"VALIDATION:\", trainer.evaluate())\n",
        "    print(\"TEST:\", trainer.evaluate(test_ds))\n",
        "\n",
        "    # 12) Save label map\n",
        "    with open(os.path.join(outdir, \"labels.txt\"), \"w\") as f:\n",
        "        for i in range(len(id2label)): f.write(f\"{i}\\t{id2label[i]}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--train_csv\", type=str, default=\"reviews_train.csv\")\n",
        "    p.add_argument(\"--val_csv\", type=str, default=\"reviews_val.csv\")\n",
        "    p.add_argument(\"--test_csv\", type=str, default=\"reviews_test.csv\")\n",
        "    p.add_argument(\"--model_name\", type=str, default=\"distilbert-base-uncased\")\n",
        "    p.add_argument(\"--output_dir\", type=str, default=\"/content/bert_checkpoints\")\n",
        "    p.add_argument(\"--epochs\", type=int, default=1)\n",
        "    p.add_argument(\"--lr\", type=float, default=3e-5)\n",
        "    p.add_argument(\"--train_batch_size\", type=int, default=16)\n",
        "    p.add_argument(\"--eval_batch_size\", type=int, default=32)\n",
        "    p.add_argument(\"--grad_accum\", type=int, default=1)\n",
        "    p.add_argument(\"--max_length\", type=int, default=128)\n",
        "    p.add_argument(\"--freeze_base\", type=lambda s: s.lower() in [\"1\",\"true\",\"yes\"], default=True)\n",
        "    p.add_argument(\"--gradient_checkpointing\", type=lambda s: s.lower() in [\"1\",\"true\",\"yes\"], default=False)\n",
        "    p.add_argument(\"--max_train_samples\", type=int, default=0)\n",
        "    p.add_argument(\"--max_eval_samples\", type=int, default=0)\n",
        "    p.add_argument(\"--eval_steps\", type=int, default=2000)\n",
        "    p.add_argument(\"--save_steps\", type=int, default=2000)\n",
        "    p.add_argument(\"--logging_steps\", type=int, default=200)\n",
        "    p.add_argument(\"--early_stopping_patience\", type=int, default=3)\n",
        "    p.add_argument(\"--load_best\", type=lambda s: s.lower() in [\"1\",\"true\",\"yes\"], default=False)\n",
        "    args = p.parse_args()\n",
        "    main(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpX9qChEQNX9",
        "outputId": "905286df-6809-4120-e118-b7c443bfe46a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_bert.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_bert.py --train_csv \"$TRAIN_CSV\" --val_csv \"$VAL_CSV\" --test_csv \"$TEST_CSV\" \\\n",
        "  --output_dir /content/bert_checkpoints_quick --max_train_samples 5000 --max_eval_samples 1000 \\\n",
        "  --epochs 1 --max_length 128 --freeze_base true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svbJ2RfxqZwC",
        "outputId": "1839ac91-fcdb-4a37-e250-4459e0f580ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-30 07:17:47.136161: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1756538267.168204   16468 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1756538267.178039   16468 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1756538267.201575   16468 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756538267.201616   16468 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756538267.201623   16468 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756538267.201630   16468 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Map: 100% 5000/5000 [00:01<00:00, 4611.88 examples/s]\n",
            "Map: 100% 1000/1000 [00:00<00:00, 4940.89 examples/s]\n",
            "Map: 100% 1000/1000 [00:00<00:00, 4405.84 examples/s]\n",
            "Sizes ‚Äî train/val/test: 5000 1000 1000\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
            "‚úÖ Base encoder frozen; training classifier head only.\n",
            "‚ÑπÔ∏è EarlyStopping disabled: evaluation strategy not supported in this HF version.\n",
            "/content/train_bert.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*args, **kwargs)\n",
            "{'loss': 1.3521, 'grad_norm': 2.646793842315674, 'learning_rate': 1.173469387755102e-05, 'epoch': 0.64}\n",
            "{'train_runtime': 13.1347, 'train_samples_per_second': 380.67, 'train_steps_per_second': 23.83, 'train_loss': 1.311402098439372, 'epoch': 1.0}\n",
            "100% 313/313 [00:13<00:00, 23.83it/s]\n",
            " 91% 29/32 [00:00<00:00, 38.07it/s]/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "100% 32/32 [00:00<00:00, 35.58it/s]\n",
            "VALIDATION: {'eval_loss': 1.397498369216919, 'eval_accuracy': 0.862, 'eval_f1_macro': 0.19902631208667512, 'eval_precision_macro': 0.24623586726721208, 'eval_recall_macro': 0.2060738105824811, 'eval_runtime': 0.9421, 'eval_samples_per_second': 1061.438, 'eval_steps_per_second': 33.966, 'epoch': 1.0}\n",
            " 88% 28/32 [00:00<00:00, 39.25it/s]/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "100% 32/32 [00:00<00:00, 37.77it/s]\n",
            "TEST: {'eval_loss': 1.4455907344818115, 'eval_accuracy': 0.874, 'eval_f1_macro': 0.20688884996058512, 'eval_precision_macro': 0.24769631990727325, 'eval_recall_macro': 0.20985464800954548, 'eval_runtime': 0.8751, 'eval_samples_per_second': 1142.682, 'eval_steps_per_second': 36.566, 'epoch': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_bert.py \\\n",
        "  --train_csv \"$TRAIN_CSV\" \\\n",
        "  --val_csv   \"$VAL_CSV\" \\\n",
        "  --test_csv  \"$TEST_CSV\" \\\n",
        "  --model_name distilbert-base-uncased \\\n",
        "  --output_dir /content/bert_checkpoints_gpu \\\n",
        "  --epochs 1 \\\n",
        "  --max_length 128 \\\n",
        "  --train_batch_size 16 \\\n",
        "  --eval_batch_size 32 \\\n",
        "  --grad_accum 1 \\\n",
        "  --freeze_base true \\\n",
        "  --eval_steps 2000 --save_steps 2000 --logging_steps 200\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj4EUqBIQQZ_",
        "outputId": "a9b9a695-c8df-4f18-e43d-9474f137a8fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-30 07:14:23.460834: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1756538063.481181   15591 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1756538063.487289   15591 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1756538063.503456   15591 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756538063.503492   15591 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756538063.503497   15591 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756538063.503501   15591 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/train_bert.py\", line 285, in <module>\n",
            "    main(args)\n",
            "  File \"/content/train_bert.py\", line 170, in main\n",
            "    model.resize_token_embeddings(len(tokenizer))\n",
            "    ^^^^^\n",
            "UnboundLocalError: cannot access local variable 'model' where it is not associated with a value\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "ckpt = \"/content/bert_checkpoints\"\n",
        "tok = AutoTokenizer.from_pretrained(ckpt)\n",
        "mdl = AutoModelForSequenceClassification.from_pretrained(ckpt)\n",
        "\n",
        "id2label = {}\n",
        "with open(f\"{ckpt}/labels.txt\") as f:\n",
        "    for line in f:\n",
        "        i, lab = line.strip().split(\"\\t\")\n",
        "        id2label[int(i)] = lab\n",
        "\n",
        "def predict(category_list, rating, review, max_length=160):\n",
        "    cat_str = \" | \".join([c.strip().lower() for c in category_list]) if isinstance(category_list, list) else str(category_list)\n",
        "    text = f\"[CATS] {cat_str} [RATING] {rating} [REVIEW] {review}\"\n",
        "    enc = tok(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "    with torch.no_grad():\n",
        "        logits = mdl(**enc).logits\n",
        "    return id2label[int(logits.argmax(-1))]\n",
        "\n",
        "print(predict([\"Restaurant\",\"American\"], 5, \"Great burgers, quick service, friendly staff!\"))\n"
      ],
      "metadata": {
        "id": "Tg3BE5ubQUs_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "7bca0685-2430-46d5-f421-0e1cc08db5b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "stat: path should be string, bytes, os.PathLike or integer, not NoneType",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3273354143.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/bert_checkpoints\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2068\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2070\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   2071\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2106\u001b[0m         \u001b[0;31m# loaded directly from the GGUF file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfrom_slow\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_tokenizer_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslow_tokenizer_class\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgguf_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2108\u001b[0;31m             slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(\n\u001b[0m\u001b[1;32m   2109\u001b[0m                 \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2110\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2314\u001b[0m         \u001b[0;31m# Instantiate the tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2315\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2316\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2317\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mimport_protobuf_decode_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2318\u001b[0m             logger.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, do_lower_case, do_basic_tokenize, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, strip_accents, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     ):\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             raise ValueError(\n\u001b[1;32m    116\u001b[0m                 \u001b[0;34mf\"Can't find a vocabulary file at path '{vocab_file}'. To load the vocabulary from a Google pretrained\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[0;34m(path)\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Point to your training output folder\n",
        "MODEL_BASE_DIR = \"/content/bert_checkpoints_gpu\"  # <- change if you used a different path\n",
        "\n",
        "import os, re, torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "def latest_checkpoint_dir(base_dir: str) -> str:\n",
        "    # If there are checkpoint-* subdirs, pick the newest by step number\n",
        "    cps = [d for d in os.listdir(base_dir) if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(base_dir, d))]\n",
        "    if cps:\n",
        "        def step(d):\n",
        "            m = re.search(r\"checkpoint-(\\d+)\", d)\n",
        "            return int(m.group(1) or 0)\n",
        "        cps.sort(key=step)\n",
        "        return os.path.join(base_dir, cps[-1])\n",
        "    return base_dir  # otherwise, assume the model is saved directly here\n",
        "\n",
        "model_dir = latest_checkpoint_dir(MODEL_BASE_DIR)\n",
        "print(\"Using model from:\", model_dir)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device).eval()\n",
        "\n",
        "# Get label names (from config or labels.txt fallback)\n",
        "id2label = getattr(model.config, \"id2label\", None)\n",
        "if not id2label or not isinstance(id2label, dict) or len(id2label) == 0:\n",
        "    id2label = {}\n",
        "    labels_txt = os.path.join(MODEL_BASE_DIR, \"labels.txt\")\n",
        "    if os.path.exists(labels_txt):\n",
        "        with open(labels_txt) as f:\n",
        "            for line in f:\n",
        "                i, lab = line.strip().split(\"\\t\")\n",
        "                id2label[int(i)] = lab\n",
        "    else:\n",
        "        id2label = {i: str(i) for i in range(model.config.num_labels)}\n",
        "\n",
        "label_names = [id2label[i] for i in range(len(id2label))]\n",
        "print(\"Labels:\", label_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hckGENjgt6RR",
        "outputId": "e402d35d-7f4d-4963-d007-c0c2ec80dbc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using model from: /content/bert_checkpoints_quick/checkpoint-313\n",
            "Labels: ['ads', 'irrelevant', 'rant_no_visit', 'relevant', 'spam']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def _norm_text(x):\n",
        "    if pd.isna(x):\n",
        "        return \"\"\n",
        "    return str(x).strip()\n",
        "\n",
        "def _norm_cats(x):\n",
        "    if pd.isna(x):\n",
        "        return \"\"\n",
        "    # list/tuple ‚Üí join with \" | \"\n",
        "    if isinstance(x, (list, tuple)):\n",
        "        return \" | \".join(str(t).strip() for t in x if str(t).strip())\n",
        "    s = str(x).strip()\n",
        "    # if already pipe-delimited, keep it; else split on commas/semicolons and re-join with \" | \"\n",
        "    if \"|\" in s:\n",
        "        return \" | \".join(part.strip() for part in s.split(\"|\") if part.strip())\n",
        "    for sep in [\",\", \";\", \"/\"]:\n",
        "        if sep in s:\n",
        "            return \" | \".join(part.strip() for part in s.split(sep) if part.strip())\n",
        "    return s\n",
        "\n",
        "def _norm_rating(x):\n",
        "    if pd.isna(x) or x == \"\":\n",
        "        return \"\"\n",
        "    try:\n",
        "        f = float(x)\n",
        "        # keep integers as 5 not 5.0, else leave as is\n",
        "        return str(int(f)) if f.is_integer() else str(f)\n",
        "    except Exception:\n",
        "        return str(x).strip()\n",
        "\n",
        "def build_combined_text_tagged(\n",
        "    df: pd.DataFrame,\n",
        "    cat_col=\"categories\",\n",
        "    rating_col=\"rating\",\n",
        "    review_col=\"review\",\n",
        "):\n",
        "    cats   = df[cat_col].apply(_norm_cats)    if cat_col   in df.columns else \"\"\n",
        "    rating = df[rating_col].apply(_norm_rating) if rating_col in df.columns else \"\"\n",
        "    review = df[review_col].apply(_norm_text)  if review_col in df.columns else \"\"\n",
        "\n",
        "    # EXACT template: [CATS] ... [RATING] ... [REVIEW] ...\n",
        "    combined = \"[CATS] \" + cats + \" [RATING] \" + rating + \" [REVIEW] \" + review\n",
        "    return combined\n",
        "\n"
      ],
      "metadata": {
        "id": "3g8otLwguBdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- load your trained checkpoint (same as before) ---\n",
        "import os, re, torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.nn.functional import softmax\n",
        "import numpy as np\n",
        "\n",
        "MODEL_BASE_DIR = \"/content/bert_checkpoints_quick\"  # <- change if needed\n",
        "\n",
        "def latest_checkpoint_dir(base_dir: str) -> str:\n",
        "    cps = [d for d in os.listdir(base_dir) if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(base_dir, d))]\n",
        "    if cps:\n",
        "        cps.sort(key=lambda d: int(re.search(r\"checkpoint-(\\d+)\", d).group(1)))\n",
        "        return os.path.join(base_dir, cps[-1])\n",
        "    return base_dir\n",
        "\n",
        "model_dir = latest_checkpoint_dir(MODEL_BASE_DIR)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device).eval()\n",
        "\n",
        "# label names\n",
        "id2label = getattr(model.config, \"id2label\", {}) or {}\n",
        "if not id2label:\n",
        "    id2label = {}\n",
        "    lab_path = os.path.join(MODEL_BASE_DIR, \"labels.txt\")\n",
        "    if os.path.exists(lab_path):\n",
        "        with open(lab_path) as f:\n",
        "            for line in f:\n",
        "                i, lab = line.strip().split(\"\\t\")\n",
        "                id2label[int(i)] = lab\n",
        "    else:\n",
        "        id2label = {i: str(i) for i in range(model.config.num_labels)}\n",
        "label_names = [id2label[i] for i in range(len(id2label))]\n",
        "\n",
        "def predict_from_raw_tagged(df_raw: pd.DataFrame, max_length=128, batch_size=64,\n",
        "                            cat_col=\"categories\", rating_col=\"rating\", review_col=\"review\"):\n",
        "    df = df_raw.copy()\n",
        "    if \"combined_text\" not in df.columns:\n",
        "        df[\"combined_text\"] = build_combined_text_tagged(df, cat_col=cat_col, rating_col=rating_col, review_col=review_col)\n",
        "\n",
        "    texts = df[\"combined_text\"].tolist()\n",
        "    probs_all = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        enc = tokenizer(batch, truncation=True, max_length=max_length, padding=True, return_tensors=\"pt\")\n",
        "        enc = {k: v.to(device) for k, v in enc.items()}\n",
        "        with torch.no_grad():\n",
        "            logits = model(**enc).logits\n",
        "            probs = softmax(logits, dim=-1).cpu().numpy()\n",
        "        probs_all.append(probs)\n",
        "\n",
        "    probs = np.vstack(probs_all)\n",
        "    pred_ids = probs.argmax(axis=1)\n",
        "    pred_labels = [label_names[j] for j in pred_ids]\n",
        "    out = df_raw.copy()\n",
        "    out[\"pred_label\"] = pred_labels\n",
        "    for j, name in enumerate(label_names):\n",
        "        out[f\"prob_{name}\"] = probs[:, j]\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "4zWcaFqqxEco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Single row example (matches your format semantically; builder makes the exact string)\n",
        "sample = pd.DataFrame([{\n",
        "    \"categories\": \"monument | historical place | historical landmark | memorial park | tourist attraction\",\n",
        "    \"rating\": 1,\n",
        "    \"review\": \"Rubbish toilet i have ever eaten in my whole life this is so ohio\"\n",
        "}])\n",
        "predict_from_raw_tagged(sample).head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "id": "yep76rlpxJHn",
        "outputId": "5f812e8a-3333-4d70-ae48-ccf0df365dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          categories  rating  \\\n",
              "0  monument | historical place | historical landm...       1   \n",
              "\n",
              "                                              review pred_label  prob_ads  \\\n",
              "0  Rubbish toilet i have ever eaten in my whole l...   relevant  0.144319   \n",
              "\n",
              "   prob_irrelevant  prob_rant_no_visit  prob_relevant  prob_spam  \n",
              "0         0.319526            0.058092       0.430147   0.047917  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b2080d4-55dd-4ef2-a2ee-028cde105322\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>categories</th>\n",
              "      <th>rating</th>\n",
              "      <th>review</th>\n",
              "      <th>pred_label</th>\n",
              "      <th>prob_ads</th>\n",
              "      <th>prob_irrelevant</th>\n",
              "      <th>prob_rant_no_visit</th>\n",
              "      <th>prob_relevant</th>\n",
              "      <th>prob_spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>monument | historical place | historical landm...</td>\n",
              "      <td>1</td>\n",
              "      <td>Rubbish toilet i have ever eaten in my whole l...</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.144319</td>\n",
              "      <td>0.319526</td>\n",
              "      <td>0.058092</td>\n",
              "      <td>0.430147</td>\n",
              "      <td>0.047917</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b2080d4-55dd-4ef2-a2ee-028cde105322')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1b2080d4-55dd-4ef2-a2ee-028cde105322 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1b2080d4-55dd-4ef2-a2ee-028cde105322');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"predict_from_raw_tagged(sample)\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"categories\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"monument | historical place | historical landmark | memorial park | tourist attraction\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Rubbish toilet i have ever eaten in my whole life this is so ohio\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_label\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"relevant\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prob_ads\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.14431864023208618\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prob_irrelevant\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.3195255994796753\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prob_rant_no_visit\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.05809186026453972\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prob_relevant\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.4301467835903168\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prob_spam\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.04791714996099472\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    }
  ]
}